{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get direct link for each post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cafebacacanarisla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>mojumojuid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>serujukcoffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>thetower_space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>solokopi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id\n",
       "0  cafebacacanarisla\n",
       "1         mojumojuid\n",
       "2      serujukcoffee\n",
       "3     thetower_space\n",
       "4           solokopi"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://www.instagram.com/'\n",
    "ig = ['cafebacacanarisla','mojumojuid','serujukcoffee','thetower_space','solokopi']\n",
    "ig = pd.DataFrame({'id':ig})\n",
    "ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_link(url, target_id):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    time.sleep(1)\n",
    "#   Login And Direct to Target URL\n",
    "    username = browser.find_element_by_xpath(\"//input[@name='username']\")    \n",
    "    username.send_keys(\"markothecow\")\n",
    "    password = browser.find_element_by_xpath(\"//input[@name='password']\")    \n",
    "    password.send_keys(\"sapikitabersama\")\n",
    "    login = browser.find_element_by_xpath(\"//button[@type='submit']\")\n",
    "    login.click()\n",
    "    \n",
    "    time.sleep(2)\n",
    "    url_target = url + target_id\n",
    "    browser.get(url_target)\n",
    "#   Scroll the page\n",
    "    body = browser.find_element_by_tag_name(\"body\")\n",
    "    no_of_pagedowns = 100\n",
    "    direct_link = []\n",
    "    while no_of_pagedowns:\n",
    "        body.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(0.2)\n",
    "        no_of_pagedowns-=1\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        card = soup.select(\"div.v1Nh3.kIKUG._bz0w\")\n",
    "        \n",
    "        for item in card:\n",
    "            post = item.find('a')\n",
    "            post = post.get('href')\n",
    "            direct_link.append(post)\n",
    "        \n",
    "    return direct_link\n",
    "\n",
    "base_url = 'https://www.instagram.com'\n",
    "ig = '/thetower_space/'\n",
    "direct = get_link(base_url, ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct = list(dict.fromkeys(direct))\n",
    "# print(direct_link)\n",
    "len(direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPSEUDOCODE 1\\n1. Untuk setiap link dalam direct_link\\n2. post_url adalah base_url + link\\n3. request post_url\\n4. scrapp data dengan try\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PSEUDOCODE 1\n",
    "1. Untuk setiap link dalam direct_link\n",
    "2. post_url adalah base_url + link\n",
    "3. request post_url\n",
    "4. scrapp data dengan try\n",
    "Error : \n",
    "\n",
    "PSEUDOCODE 2\n",
    "1. Scrap saat selenium membuka\n",
    "2. return dalam bentuk data frame/list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert list to String\n",
    "def to_string(lst): \n",
    "      \n",
    "    return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = []\n",
    "caption_list = []\n",
    "likes_list = []\n",
    "hashtag_list = []\n",
    "loc_list = []\n",
    "photo_list = []\n",
    "\n",
    "def scrapper_post():    \n",
    "    browser = webdriver.Chrome()\n",
    "    for link in direct:\n",
    "        url = base_url + link\n",
    "        browser.get(url)\n",
    "        html = browser.page_source\n",
    "        \n",
    "        ## Scrapp data ##\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        ### Upload date ###\n",
    "        date_upload = soup.find(\"time\")\n",
    "        date_upload = date_upload.get('datetime')\n",
    "        upload.append(date_upload)\n",
    "        ### Caption ###\n",
    "        caption = soup.find('div', attrs={\"class\": \"C4VMK\"})\n",
    "        caption = caption.find('span')\n",
    "        caption = caption.get_text()\n",
    "        caption_list.append(caption)\n",
    "        ### Likes & Views ###\n",
    "        try:\n",
    "            likes = soup.find('button', attrs={\"class\": \"sqdOP yWX7d _8A5w5\"})\n",
    "            likes = likes.get_text()\n",
    "            likes_list.append(likes)\n",
    "        except:\n",
    "            views = soup.find('span', attrs={\"class\": \"vcOH2\"})\n",
    "            views = views.get_text()\n",
    "            likes_list.append(views)\n",
    "        ### Hashtag ###\n",
    "        tag_list = []\n",
    "        tags = soup.find('div', attrs={\"class\": \"C4VMK\"})\n",
    "        tags = tags.find_all('a', attrs={\"class\": \"xil3i\"})\n",
    "        for tag in tags:\n",
    "            x = tag.get_text()\n",
    "            tag_list.append(x)\n",
    "        tag_string = to_string(tag_list)\n",
    "        hashtag_list.append(tag_string)\n",
    "        ### Location ###\n",
    "        try:\n",
    "            loc = soup.find('a', attrs={\"class\": \"O4GlU\"})\n",
    "            loc = loc.get('href')\n",
    "            loc = base_url + loc\n",
    "            loc_list.append(loc)\n",
    "        except:\n",
    "            loc = ''\n",
    "            loc_list.append(loc)\n",
    "        ### Photos ###\n",
    "        try:\n",
    "            photo = soup.find('div', attrs={\"class\": \"KL4Bh\"})\n",
    "            photo = photo.find('img')\n",
    "            photo = photo.get('src')\n",
    "            photo_list.append(photo)\n",
    "        except:\n",
    "            photo = soup.find('video')\n",
    "            photo = photo.get('src')\n",
    "            photo_list.append(photo)\n",
    "        \n",
    "    dic = {'upload':upload,'caption':caption_list,'likes_list':likes_list,\n",
    "           'hashtag':hashtag_list,'location':loc_list,'photo':photo_list}\n",
    "    df = pd.DataFrame(dic)\n",
    "    return df\n",
    "\n",
    "csv = scrapper_post()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cafebacacanarisla',\n",
       " 'mojumojuid',\n",
       " 'serujukcoffee',\n",
       " 'thetower_space',\n",
       " 'solokopi']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv.to_csv(r'dataset/post-thetower_space.csv', index = False)\n",
    "\n",
    "ig = ['cafebacacanarisla','mojumojuid','serujukcoffee','thetower_space','solokopi']\n",
    "ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = '/p/B_7FEU-hfWq/'\n",
    "x = base_url + link\n",
    "def html_post(url):    \n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    html = browser.page_source\n",
    "    return html\n",
    "\n",
    "html = html_post(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://instagram.fjog4-1.fna.fbcdn.net/v/t51.2885-15/e35/p1080x1080/96086942_110222837193191_6328433041036236771_n.jpg?_nc_ht=instagram.fjog4-1.fna.fbcdn.net&_nc_cat=111&_nc_ohc=6_rbOPMZeVAAX9Shi-0&oh=5f50d1716db54bb33f44d84b846669f7&oe=5F3D5B9F'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html,'html.parser')\n",
    "## Uploap date\n",
    "date_upload = soup.find(\"time\")\n",
    "date_upload = date_upload.get('datetime')\n",
    "\n",
    "## Caption\n",
    "caption = soup.find('div', attrs={\"class\": \"C4VMK\"})\n",
    "caption = caption.find('span')\n",
    "caption = caption.get_text()\n",
    "\n",
    "## Likes & Views\n",
    "likes = soup.find('button', attrs={\"class\": \"sqdOP yWX7d _8A5w5\"})\n",
    "likes = likes.get_text()\n",
    "# views = soup.find('span', attrs={\"class\": \"vcOH2\"})\n",
    "# views = views.get_text()\n",
    "\n",
    "## tags\n",
    "tag_list = []\n",
    "tags = soup.find('div', attrs={\"class\": \"C4VMK\"})\n",
    "tags = tags.find_all('a', attrs={\"class\": \"xil3i\"})\n",
    "for tag in tags:\n",
    "    x = tag.get_text()\n",
    "    tag_list.append(x)\n",
    "to_string(tag_list)\n",
    "\n",
    "## Location\n",
    "loc = soup.find('a', attrs={\"class\": \"O4GlU\"})\n",
    "loc = loc.get('href')\n",
    "loc = base_url + loc\n",
    "\n",
    "## Photos\n",
    "photo = soup.find('div', attrs={\"class\": \"KL4Bh\"})\n",
    "photo = photo.find('img')\n",
    "photo = photo.get('src')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
